# 实现错误处理与监控体系：DLQ 与 Metrics 实战

## 1. 背景与目标

在构建基于 Redis Stream 的异步消息处理系统（如会话归档、文档 ETL）时，我们面临两大挑战：

1. **消息处理失败怎么办？** 如果消费者抛出异常，不能无限重试阻塞队列，也不能直接丢弃数据。
2. **系统健康度如何感知？** 我们需要知道当前积压了多少消息（Lag），以及 Redis 的写入性能是否下降。

**解决方案**：

* **错误兜底**：引入 **死信队列 (Dead Letter Queue, DLQ)**。
* **全链路监控**：基于 Micrometer 实现 **Stream Lag**、**DLQ Size** 和 **P99 Latency** 监控。

---

## 2. 错误处理：死信队列 (DLQ) 设计

### 2.1 核心机制

当消费者（Consumer）处理消息失败，且重试次数超过阈值（如 3 次）时，执行以下“降级”操作：

1. **ACK 确认**：在原 Stream 中确认该消息（XACK），防止阻塞后续消息。
2. **转移存储**：将消息 Payload 推入一个专门的 Redis List (`aidemo:session:dlq`)。
3. **人工介入**：运维人员定期检查 DLQ，修复数据或 Bug 后重新投递。

### 2.2 告警机制 (`DlqMonitorTask`)

我们不采用实时监听 DLQ 的方式（成本过高），而是采用 **“定时巡检”** 策略。

* **组件**：`DlqMonitorTask`
* **频率**：每 5 分钟执行一次。
* **逻辑**：
    1. 检查 Redis List `aidemo:session:dlq` 的长度。
    2. 如果长度 > 0，打印 `ERROR` 级别日志。
    3. **采样**：读取第一条死信内容并打印，方便运维快速定位问题。

> **运维侧配合**：ELK 或 Prometheus 需配置日志告警规则，匹配 `🚨 [CRITICAL] 死信队列告警` 关键字即发送钉钉/邮件通知。

---

## 3. 监控指标体系 (Metrics)

我们使用 Spring Boot Actuator + Micrometer 构建监控指标，最终由 Prometheus 拉取并展示在 Grafana 上。

### 3.1 消费积压监控 (Stream Lag)

* **痛点**：Spring Boot 默认不提供 Redis Stream 的 Lag 指标。
* **实现**：`RedisStreamMetrics` (实现 `MeterBinder` 接口)。
* **原理**：
    * 注册一个 `Gauge` (仪表盘)。
    * 当 Prometheus 拉取数据时，动态执行 `XINFO GROUPS` 命令获取 `Lag` 值。
* **指标名**：`aidemo.redis.stream.lag`
* **Tags**：`stream`, `group`

### 3.2 死信堆积监控 (DLQ Size)

* **目的**：在 Grafana 大盘上直观展示死信数量的变化趋势。
* **实现**：`RedisDlqMetrics`。
* **原理**：注册 `Gauge`，动态执行 `LLEN` 命令。
* **指标名**：`aidemo.redis.dlq.size`

### 3.3 业务成功率监控

* **目的**：统计归档任务的成功率。
* **实现**：在 `SessionArchiveService` 中埋点 `Counter`。
* **指标名**：
    * `aidemo.session.archive.success` (成功数)
    * `aidemo.session.archive.error` (失败数)
* **计算公式**：`Rate(error) / (Rate(success) + Rate(error))`

### 3.4 Redis 写入延迟 (P99 Latency)

* **目的**：感知 Redis 是否变慢（如发生 AOF 重写阻塞）。
* **实现**：
    1. **注解** `@RedisWriteLatency`：标记关键写入方法（如心跳更新）。
    2. **切面** `RedisLatencyAspect`：拦截方法，记录耗时。
    3. **配置**：显式开启直方图统计 (`percentiles(0.5, 0.95, 0.99)`)。
* **指标名**：`aidemo.redis.write.latency`

---

## 4. 监控大盘配合策略

在 Grafana 中，我们应将 **Stream Lag** 和 **DLQ Size** 放在同一个 Row 中对比观察，这能帮助我们快速定位故障类型。

| 现象         | Stream Lag (积压) | DLQ Size (死信) | 诊断分析                                | 应对措施                                |
|:-----------|:----------------|:--------------|:------------------------------------|:------------------------------------|
| **正常**     | 低 (<10)         | 0             | 系统健康运行                              | 无                                   |
| **性能瓶颈**   | **高** (持续增长)    | 0             | 消费者处理速度 < 生产速度。可能是数据库慢、CPU 瓶颈。      | 1. 增加消费者实例 (扩容)<br>2. 优化消费逻辑 (批量写入) |
| **代码 Bug** | 低               | **高** (突增)    | 消费者处理极快，但都在报错（如空指针），导致消息快速流入死信队列。   | 1. 查看死信日志样本<br>2. 回滚代码或修复 Bug       |
| **级联故障**   | **高**           | **高**         | 错误导致大量重试（Retry），重试占用了消费时间，导致后续消息积压。 | 优先解决报错问题，Lag 通常会随之缓解。               |

---

## 5. 附录：关键代码片段

### 5.1 DLQ 监控任务

```java

@Scheduled(cron = "0 0/5 * * * ?")
public void checkDeadLetterQueue() {
    Long size = redisTemplate.opsForList().size(DLQ_KEY);
    if (size != null && size > 0) {
        logger.error("🚨 [CRITICAL] 死信队列告警! 当前堆积: {}", size);
    }
}
```

### 5.2 Stream Lag 指标

```java
Gauge.builder("aidemo.redis.stream.lag",this,metrics ->{
StreamInfo.XInfoGroups groups = redisTemplate.opsForStream().groups(STREAM_KEY);
    return groups.

stream()
            .

filter(g ->g.

groupName().

equals(GROUP_NAME))
        .

findFirst().

map(StreamInfo.XInfoGroup::lag).

orElse(0L);
}).

register(registry);
```

---
*文档生成时间: 2024-05-21*